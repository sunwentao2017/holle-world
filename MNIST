import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
%matplotlib inline  

from tensorflow.examples.tutorials.mnist import input_data            #导入MNIST数据集 
mnist = input_data.read_data_sets('/tmp/data/', one_hot=True)


### 下面观察一下这个dataset ###

print (" tpye of 'mnist' is %s" % (type(mnist)))
print (" number of trian data is %d" % (mnist.train.num_examples))
print (" number of test data is %d" % (mnist.test.num_examples))


trainimg   = mnist.train.images                                    # What does the data of MNIST look like? 
trainlabel = mnist.train.labels
testimg    = mnist.test.images
testlabel  = mnist.test.labels
print
print (" type of 'trainimg' is %s"    % (type(trainimg)))             
print (" type of 'trainlabel' is %s"  % (type(trainlabel)))
print (" type of 'testimg' is %s"     % (type(testimg)))
print (" type of 'testlabel' is %s"   % (type(testlabel)))
print (" shape of 'trainimg' is %s"   % (trainimg.shape,))
print (" shape of 'trainlabel' is %s" % (trainlabel.shape,))
print (" shape of 'testimg' is %s"    % (testimg.shape,))
print (" shape of 'testlabel' is %s"  % (testlabel.shape,))


                                                                    # How does the training data look like?
nsample = 5
randidx = np.random.randint(trainimg.shape[0], size=nsample)        # 随机看五个图

for i in randidx:
    curr_img   = np.reshape(trainimg[i, :], (28, 28)) # 28 by 28 matrix 
    curr_label = np.argmax(trainlabel[i, :] ) # Label
    plt.matshow(curr_img, cmap=plt.get_cmap('gray'))
    plt.title("" + str(i) + "th Training Data " 
              + "Label is " + str(curr_label))
    print ("" + str(i) + "th Training Data " 
           + "Label is " + str(curr_label))
           
           
                                                                    # Batch Learning
batch_size = 100                                                   
batch_xs, batch_ys = mnist.train.next_batch(batch_size)
print ("type of 'batch_xs' is %s" % (type(batch_xs)))
print ("type of 'batch_ys' is %s" % (type(batch_ys)))
print ("shape of 'batch_xs' is %s" % (batch_xs.shape,))
print ("shape of 'batch_ys' is %s" % (batch_ys.shape,))

randidx   = np.random.randint(trainimg.shape[0], size=batch_size)   # Get Random Batch with 'np.random.randint'
batch_xs2 = trainimg[randidx, :]
batch_ys2 = trainlabel[randidx, :]
print ("type of 'batch_xs2' is %s" % (type(batch_xs2)))
print ("type of 'batch_ys2' is %s" % (type(batch_ys2)))
print ("shape of 'batch_xs2' is %s" % (batch_xs2.shape,))
print ("shape of 'batch_ys2' is %s" % (batch_ys2.shape,))

print(randidx)                                                      # 看一下随机取的结果
print(batch_ys2)

### 下面是正式代码 ###


                                            # 输入变量，把28*28的图片变成一维数组（丢失结构信息）  
x = tf.placeholder("float",[None,784])  
  
                                            # 权重矩阵，把28*28=784的一维输入，变成0-9这10个数字的输出  
w = tf.Variable(tf.zeros([784,10]))  
                                            # 偏置  
b = tf.Variable(tf.zeros([10]))  
  
                                            # 核心运算，其实就是softmax（x*w+b）  
y = tf.nn.softmax(tf.matmul(x,w) + b)  
  
                                            # 这个是训练集的正确结果  
y_ = tf.placeholder("float",[None,10])  
  
                                            # 交叉熵，作为损失函数  
cross_entropy = -tf.reduce_sum(y_ * tf.log(y))  
  
                                            # 梯度下降算法，最小化交叉熵  
train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)  
  
                                            # 初始化，在run之前必须进行的  
init = tf.initialize_all_variables()  
                                            # 创建session以便运算  
sess = tf.Session()  
sess.run(init)  
  
                                            # 迭代1000次  
for i in range(1000):  
                                            # 获取训练数据集的图片输入和正确表示数字  
  batch_xs, batch_ys = mnist.train.next_batch(100)  
                                            #运行刚才建立的梯度下降算法，x赋值为图片输入，y_赋值为正确的表示数字  
  sess.run(train_step,feed_dict = {x:batch_xs, y_: batch_ys})  
  
                                            # tf.argmax获取最大值的索引，比较运算后的结果和本身结果是否相同。   
                                            # 返回一组布尔值，1代表正确，0代表错误  
correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))  
  
                                            # tf.cast先将数据转换成float，防止求平均不准确。  
                                            # tf.reduce_mean由于只有一个参数，就是上面那组布尔值的平均值。  
accuracy = tf.reduce_mean(tf.cast(correct_prediction,"float"))  
                                            # 输出正确率0.913
print sess.run(accuracy,feed_dict={x:mnist.test.images,y_: mnist.test.labels})  
